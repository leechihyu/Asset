<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GPT Illustration</title>
    <link rel="stylesheet" href="style.css">
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript"
    src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
</head>
<body>
    <div id="sidebar" class="sidebar">
        <button id="sidebar-toggle" class="sidebar-toggle">&#9776;</button>
        <nav>
            <ul>
                <li><a href="#model-structure">大语言模型结构及工作机制</a></li>
                <li><a href="#model-body">模型身子：Transformer模块</a></li>
                <li><a href="#training">数据及模型训练</a></li>
                <li><a href="#ops">大模型运营</a></li>
                <li><a href="#security">额外话题：内容安全</a></li>
                <li><a href="#other">其它话题</a></li>
            </ul>
        </nav>
    </div>
    <div class="container">

    <div class="post-header">
    <h1 class="post-title">写给社会科学家的大模型指南</h1>
    <div class="post-meta">
        <span class="post-author">作者：Zhiyu Li</span>
        <span class="post-date">2025年8月8日</span>
    </div>
</div>

    <p>本文旨在为希望了解大语言模型工作机制的社会科学研究者提供一个稍有深度、而不失广度且详略得当的介绍。</p>

    <p>许多社会科学研究者对于大语言模型，尤其是可被大众广泛接触的基于大语言模型的聊天机器人颇有兴趣。然而，大多数现有介绍要么停留在抽象的技术概述（如注意力机制、自回归原理），要么范围过窄，忽略了链式思维（Chain-of-Thought, CoT）、模型运营（LLMOps）以及最新研究进展等对社会研究同样重要的内容。</p>

    <p>仅仅了解大语言模型的大致工作原理对于一般大众来说或许足够，但是对于有志于深入研究大语言模型的社会影响，或其承载的社会意义等话题的研究者来说太过浅显。本文旨在弥补这一空缺。本文会结合代码介绍大语言模型的架构、结合论文研究介绍大语言模型的最新发现、结合一般业界经验介绍大语言模型是如何部署的。当然，这几点并非大语言模型运营以及研究的全部，其它话题，例如如何更新模型权重、如何提高模型的推理速度、如何理解模型参数中的特征叠加现象等等，虽然同样重要，但是对于社会科学研究者来说意义不大，因此聊以省略。总结来说，本文会介绍以下三个部分：

    <ol>
      <li>大语言模型的架构以及模型本身的工作机制（模型）；</li>
      <li>数据及后训练，包括监督式微调和基于人类反馈的强化学习（数据）；</li>
      <li>大语言模型运营，即人工智能企业如何部署模型让其可供大众使用。（LLMOps）</li>
    </ol>
    
    <p>笔者相信，了解以上三部分可以为有关大语言模型的社会科学研究打下坚实的基础。</p>

    <h2 id="model-structure">大语言模型结构及工作机制</h2>
    
    <p>抽象来说，大语言模型可以被视为一个随机函数：输入是用户的文本，输出是模型生成的回答。这里的“随机”指的是，模型不会每次都给出完全相同的答案，而是根据概率分布进行采样。举个简单的例子，假设用户输入的文本经过分词（tokenization）后共有 10 个 token，而模型最终生成了 20 个新的 token，那么在生成的过程中，模型相当于进行了 20 轮迭代：第一次迭代的输入是用户提供的 10 个 token，输出是第 1 个新 token；第二次迭代则以上一轮的全部 11 个 token 为输入，输出第 2 个新 token……以此类推。当模型生成了特殊的结束符（如 &lt;END&gt;）或达到最大 token 限制时，就会停止生成。这种“不断以上一步的全部输出为下一步的输入、逐步预测下一个 token”的方式被称为自回归（autoregression）。采用这种方式的模型也叫因果语言模型（causal language model）或生成式预训练模型（GPT）。顺便一提，仅经过预训练的模型往往缺乏判断何时停止的能力，可能会把所有与输入相关的内容一股脑输出，直到触碰最大长度限制——这一点我们在后文还会再提到。</p>

    <p>从总体层面看，大语言模型可以看作一个随机函数：给定输入，它会生成一个输出分布。但它远不像计算f(x)=2x那样直接——它内部包含了庞大的前馈网络结构，涉及数以亿计、甚至上百亿的可训练参数。这个“随机函数”是如何被定义出来的？答案藏在它的架构里。接下来我们就来看看。</p>

    <p>我会结合代码讲解大语言模型的核心架构——解码器架构（decoder-only model）。先回答一个可能的疑问：这个架构值得逐行代码去学吗？我的答案是：值得。解码器模型目前在业界占据绝对主导地位，几乎没有架构能与之正面竞争。GPT 系列、LLaMA 系列、Mixtral、DeepSeek R1 都是解码器架构的变种。经典的 GPT 使用稠密解码器；DeepSeek R1 则是混合专家（MoE, Mixture of Experts）的稀疏解码器架构。OpenAI 最近也开源了<a href="https://huggingface.co/openai">两个MoE模型</a>。简单来说，MoE 的总参数量比稠密模型大几倍，但每次推理只激活少数“专家”子网络，因此推理速度反而更快，同时还能在相同算力下训练更大规模、更多数据的模型。综合当前趋势，MoE 或其它形式的稀疏模型很可能会在未来占据重要地位，尤其是在硬件与软件优化不断进步的前提下。</p>
        
    <p>目前在 Transformer 领域，最受关注的潜在挑战者是 2023 年提出的 <a href="https://arxiv.org/abs/2312.00752">Mamba架构</a>，这是一种完全不同于解码器的设计思路。它在发布初期曾引发热烈讨论，但随后并未出现重大突破。近期，英伟达也发布了<a href="https://research.nvidia.com/publication/2025-06_mambavision-hybrid-mamba-transformer-vision-backbone">关于 Mamba 的研究</a>，不过方向是将其与 Transformer 结合使用。现实问题在于，Mamba 目前既缺乏为其优化的 GPU/TPU 等硬件支持，也缺乏成熟的生态系统。在解码器架构的 Scaling Law 潜力尚未被完全挖掘之前，大型科技公司缺乏投入巨额资源押注 Mamba 的动机。因此，从实用角度看，当前仍以解码器架构为重点即可。而且，解码器模型与编码器模型在核心机制上高度相似，掌握解码器的工作原理，也就为理解包括编码器在内的 Transformer 架构打下了坚实基础。</p>

    <p>解码器模型由 N 个完全相同的模块堆叠而成。可以把它想象成：一个负责处理文字的“头”，一个由 N 层相同结构组成的“身子”，以及一个将数字再转换回文字的“尾巴”。</p>

    <p text-align="center">
        <img src="https://raw.githubusercontent.com/leechihyu/Asset/c6a9c209d26e431ec76f2fb9c7b82be8dc5f1fe8/llm_illustration/fig3.png" alt="GPT Full Architecture" width="400">
    </p>

    <p><a href="https://medium.com/@anitishkoffice/historical-context-seq2seq-paper-and-nmt-by-joint-learning-to-align-translate-paper-da0f6f2fe939">Source</a></p>

    <p>
        我们先来看“头”的部分。因为计算机只能处理数字，神经网络的计算本质上就是矩阵运算，所以模型的第一步，就是把输入的文字转成数字向量。更准确地说，这一步需要完成两个任务：一、将token转成向量；二、添加位置信息。具体怎么做呢？</p>

    <p>在进入模型之前，文本会先经过 tokenizer（分词器）。它会自动把文本切分成模型的基本“词表单元”，并查表找到对应的编号。例如，在 DeepSeek 的词表中，“Apple” 可能对应编号 26567。中文的切分方式不完全相同：有的词表会将每个汉字当作一个 token，有的则会将“人工智能”这样的词整体作为一个 token。不同语言和不同模型的词表设计都会影响切分方式。本文不会深入介绍 tokenizer 的构建过程，因为这与大多数社会科学研究关系不大，但理解它的功能对于认识模型的输入处理是有帮助的。</p>

    <h3>模型头</h3>

    <p><strong>将token转成向量</strong></p>
    <pre>
        <code>
            import torch
            import torch.nn as nn
            import math

            class InputEmbeddings(nn.Module):

                def __init__(self, d_model: int, vocab_size: int) -> None:
                    super().__init__()
                    self.d_model = d_model
                    self.vocab_size = vocab_size
                    self.embedding = nn.Embedding(vocab_size, d_model)

                def forward(self, x):
                    # (batch, seq_len) --> (batch, seq_len, d_model)
                    # Multiply by sqrt(d_model) to scale the embeddings according to the paper
                    return self.embedding(x) * math.sqrt(self.d_model)
        </code>
    </pre>

    <p>第一步是把文字转成向量。这里的 nn.Embedding 可以理解成一张“查表”——词表里有多少个 token，就有多少行，每行是一个长度为 d_model 的向量。</p>

    <li>vocab_size：词表大小（token 总数）</li>

    <li>d_model：模型维度，即每个 token 向量的长度</li>

    <p>训练的过程中，模型会自动学习这个表中的参数，让每个 token 对应的向量能携带它的语义特征。</p>

    <p><strong>添加位置信息</strong></p>
    
    <pre>
        <code>
        class PositionalEncoding(nn.Module):

            def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:
                super().__init__()
                self.d_model = d_model
                self.seq_len = seq_len
                self.dropout = nn.Dropout(dropout)
                # Create a matrix of shape (seq_len, d_model)
                pe = torch.zeros(seq_len, d_model)
                # Create a vector of shape (seq_len)
                position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)
                # Create a vector of shape (d_model)
                div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)
                # Apply sine to even indices
                pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))
                # Apply cosine to odd indices
                pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))
                # Add a batch dimension to the positional encoding
                pe = pe.unsqueeze(0) # (1, seq_len, d_model)
                # Register the positional encoding as a buffer
                self.register_buffer('pe', pe)

            def forward(self, x):
                x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)
                return self.dropout(x)
        </code>
    </pre>
    
    <p>仅有词向量还不够，因为语言是有顺序的。同一个词出现在不同位置时，含义可能会改变。位置嵌入（Positional Encoding）就是用来告诉模型“这个词在句子里的位置”。一种常见做法是使用固定的正弦和余弦函数（sin/cos）生成位置向量，公式是：</p>

    <p>$PE_{(pos, 2i)}=sin\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)$</p>

    <p>$PE_{(pos, 2i+1)}=cos\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)$</p>

    <p>当然，我们也可以让模型自己去学如何给位置赋予信息，这种方法会更加灵活。但是相应的，会增加更多计算量。如果学一个位置编码器的话，这个编码器将由<code>max_seq_len * d_model</code>个参数组成。我们需要注意到在<code>forward(...)</code>函数里，定义了模型如何处理词嵌入，并且为模型的下一层返回什么内容。</p>

    <pre>
        <code>
        def forward(self, x):
            x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)
            return self.dropout(x)
        </code>
    </pre>

    <p>以上代码展示了位置嵌入层的工作流程，位置嵌入层接受词嵌入的输出，形状为<code>(1, n_seq, d_model)</code>。在算出每个位置上每一个维度对应的位置嵌入之后，将位置信息加入词嵌入之中，然后返回一个随机抽离部分内容的结果（这一步的目的是防止过拟合）。</p>

    好，到这时我们就有了一个张量，其维度依旧为<code>(1, n_token, d_model)</code>下面就进入到非常关键的部分，也就是transformer模块。
    
    <h3 id="model-body">模型身子：Transformer模块$\times N$ </h3>

    <p>Transformer 模块是大语言模型的核心，而其中的多头注意力机制（Multi-Head Attention）又是核心中的核心。这一机制最早出现在论文<a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>中，名字里的 “Attention” 指的就是它。</p>

    <img src="https://raw.githubusercontent.com/leechihyu/Asset/9ed080389a9994f63ee1e03fd229736df1156d3c/llm_illustration/fig4.png" width="500">

    <p><a href="https://en.wikipedia.org/wiki/Attention_(machine_learning)">Source</a></p>

    <p>直觉上，注意力机制的目标是：找到与当前 token 最相关的信息，并用这些信息来预测它的输出。举个例子。我们来看一个小例子：</p>

    <pre><code>汤姆是一个可爱的小男孩，我们都喜欢_</code></pre>

    <p>如果要预测下划线处的词，人类会自动关注“汤姆”“可爱的”“小男孩”这几个关键信息。前者给出名字，后者提供情感与描述，最后一个告诉我们性别属性。</p>
    
    <p>机器也需要做类似的事：</p>

    <ol>1. 确定当前我们要预测的 token（即 Query）</ol>

    <ol>2. 在已有的上下文中找出和它最相关的 token（通过 Key 计算注意力分数</ol>

    <ol>3. 把相关 token 的信息整合起来（通过 Value 加权求和）</ol>

    <p>在形式化表示中，我们会把每个输入 token 的词向量（Embedding）通过线性变换分别得到 Q（查询向量）、K（键向量）、V（值向量）：</p>

    <li>Q：用来代表“我现在在问什么”</li>

    <li>K：用来回答“你能匹配我这个问题吗”</li>

    <li>V：用来提供匹配到的信息内容</li>

    <p>注意力分数由 Q 和 K 的相似度计算得出，这些分数会作为权重加到 V 上，从而得到融合了上下文信息的向量，供后续预测使用。多头注意力（Multi-Head Attention）只是把这个过程并行做多组，让模型可以同时关注不同类型的关联。</p>

    <p>Attention Score = $Softmax(\frac{QK^{T}}{\sqrt{d_{k}}})$</p>

    <p>在有了attention score之后，就可以对value进行加权了。$Attention(Q,K,V)=Softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V$</p>

    <p>在 Transformer 中，Q、K、V 并不是凭空出现的，而是由模型自己学出来的。具体来说，每个 token 进入注意力机制时，都会先有一个形状为 (1, d_model) 的向量（也就是它的 embedding）。在 自注意力机制（self-attention） 中，Q、K、V 都是由这个向量本身生成的，只是各自经过了不同的线性变换。这三个线性变换分别由可训练的矩阵$W_{Q}$, $W_{K}$, $W_{V}$定义：

        <li>Q=embedding$\times W_{Q}$</li>

        <li>K=embedding$\times W_{K}$</li>

        <li>V=embedding$\times W_{V}$</li>
        
    <p>因为$QW_{Q}$,$W_{K}$,$W_{V}$的参数在训练过程中不断更新，模型就能逐渐学会如何让 Q、K、V 在各自的角色中“各司其职”：</p>

        <li>Q更适合去提问、寻找关联</li>

        <li>K更适合提供匹配条件</li>

        <li>V更适合传递上下文的语义信息</li>

    <p>换句话说，自注意力中的 Q、K、V 虽然都源于同一个输入向量，但通过不同的投影矩阵，它们学会了分工协作，从而让模型能够计算注意力分数并融合上下文信息。</p>到这里，我们就把注意力机制的基本原理讲完了。但它还有一个非常实用的特性——可以并行计算。在大多数架构图中，你会看到注意力层被标成 MultiHeadAttention，这就是“多头注意力”的意思。具体做法是：我们沿着d_model这一维度，将d_model平均分成n份，比方说d_model为2048，我们就可以将它平均分成32份，每一份都有64个维度。我们就说这个注意力机制有32个头。每个注意力头会独立地执行完整的注意力计算——它会用自己的 Q、K、V 算注意力分数，再用分数加权 V。这样，每个头就像是在从不同的角度、不同的特征空间去“看”输入序列。等所有头都算完后，把它们的结果拼接起来，形成一个新的向量，这个向量就综合了多种关联信息。这就是“多头注意力”名字的由来，也是它能同时捕捉多种关系、并行运算的原因。</p>

    <p>前馈层（Feed-Forward Network, FFN）是注意力机制之后的核心组成部分。多头注意力会将d_model维的向量切分成多个头分别处理，这样确实可能临时“割裂”不同特征之间的联系。不过，在进入前馈层之前，这些头的输出会重新拼接回一个完整的d 
model维向量，前馈层会在整个向量空间上进行非线性变换，从而让不同特征重新交互，提炼更高层次的表示。为了提升模型容量，前馈层通常设计得比d_model大得多，例如 4 倍甚至更多，这也使得它占据了大量参数。值得一提的是，稠密解码器和 MoE（Mixture of Experts）架构的主要差异，就在于前馈层：稠密模型使用单一的全连接前馈层，而 MoE 则引入多个“专家”网络，并在每次推理时只激活其中一小部分，实现稀疏计算。</p>

    <p>在前馈层之后，会有两个常见的结构来提升训练稳定性：Layer Normalization（层归一化）用于规范化每个样本的特征分布，缓解梯度爆炸或梯度消失问题；残差连接（<a href="https://arxiv.org/abs/1512.03385">ResNet 结构</a>）则直接将模块的输入与输出相加，让信息能够“绕过”复杂变换，帮助深层网络更容易训练。</p>

    <p>这样的“多头注意力 → 前馈层 → 标准化与残差”结构会作为一个整体模块反复堆叠N次，构成解码器的主体。层数越多，模型的表达能力通常越强，但计算成本和显存需求也会成倍增加。</p>

    <p>经过 N 层解码器模块的处理，我们最终得到一个形状为(n_seq, d_model) 的表示矩阵，它在数学意义上已经浓缩了输入上下文中的所有关键信息。接下来，模型需要把这种“语义向量”转化为具体的词（token）。这一步就是投影层（projection layer）的工作：它会将每个位置的向量映射到一个长度为vocab_size 的向量，其中每个分量（logit）代表生成该 token 的相对倾向。再经过 softmax 运算，就能得到一个概率分布，从而选出最可能的下一个 token。下面的代码展示了这个投影层的实现，它本质上就是一层简单的线性映射：</p>

    <pre>
        <code>
        class ProjectionLayer(nn.Module):
            def __init__(self, d_model, vocab_size) -> None:
                super().__init__()
                self.proj = nn.Linear(d_model, vocab_size)

            def forward(self, x) -> None:
                # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)
                return self.proj(x)
        </code>
    </pre>

    <p>注意力机制的一个天然缺点是计算和显存开销都很大。因为它会在生成过程中，对输入序列中的每一个 token 与其他所有 token 计算注意力分数，这使得时间复杂度随序列长度呈平方增长($O(n^{2})$)。如果实现没有针对性优化，还需要一次性把全部 token 放入显存（VRAM），显存压力会非常大，因此在实际部署中，模型往往会设置一个最大输入长度。此外，即便计算能力充足，原生解码器模型还存在 <a href="https://arxiv.org/abs/2307.03172">U 型注意力偏差</a>（U-shaped attention bias）——中间部分的文本往往得不到足够关注；同时，研究还发现了 <a href="https://arxiv.org/abs/2507.22887">positional bias</a>（位置偏差）现象，即信息在序列中的位置会显著影响模型的输出质量。目前，业界主要有三种途径来缓解长文本处理的限制：</p>

        <li>通过增加模型参数、改进显存管理与推理速度（如分布式推理、KV 缓存优化等）来容纳更长上下文，同时配合算法手段削弱位置偏差。</li>
        
        <li><a href="https://arxiv.org/abs/2005.11401">RAG（Retrieval-Augmented Generation）</a>。将长文本分割成小段，通过检索选出最相关的部分输入模型，从而降低计算量。</li>
        
        <li>采用 Mamba 等非 Transformer 架构。Mamba 在长序列推理中的时间复杂度是线性的$O(n)$，显著优于 Transformer 的平方复杂度，更适合长文本任务。</li>
    
    <h2 id="training">数据及模型训练</h2>

    <p>相比模型架构的讨论，模型训练的脉络反而更容易看清。架构的革命往往是突如其来的——是 Transformer 继续领跑，還是 Mamba 异军突起，抑或是最近引人注目的 Hierarchical Reasoning Model（HRM）走出新路，目前没人能给出确定答案。但在训练方法上，研究者和从业者似乎有了一个共同方向，因为大家对 AI 的长远愿景有某种共识，也大致知道要实现这个愿景，模型需要具备哪些能力。OpenAI 曾提出一个有趣的“五层能力”框架：</p>

    <li>层次一：聊天机器人</li>
    <li>层次二：推理模型</li>
    <li>层次三：Agent</li>
    <li>层次四：创新者</li>
    <li>层次五：组织者</li>

    <p>我们目前大致处在第三层——Agent——这一阶段的竞争异常激烈。回顾前两个阶段，聊天机器人的突破很大程度上得益于大规模预训练和有监督微调（SFT）。GPT-3 就已经能生成看似不错的内容，但并未真正“惊艳”；随着 scaling law 的持续发挥作用，GPT-3.5 一举突破了那个“让人惊叹的阈值”。不少研究表明，大语言模型的大部分核心能力来自预训练。所谓预训练，就是在海量文本数据上让模型学习预测下一个 token，这一过程中还要对数据进行过滤，确保预测目标是有意义的。</p>

    <p>预训练让模型像一个“博闻强记但不会说好话的人”——知识丰富，但不会根据场合恰当地组织语言。未经过微调的模型常会“一股脑”地输出所有相关信息，不管这些信息是不是用户真正需要的，甚至不太知道该何时停下。SFT 的作用，就是用带有人类期望的问答数据去教它“好好说话”。这些数据有的从现有文本中编撰，有的由人类直接撰写。在这个过程中，还可以用知识蒸馏的方法，让一个大模型充当老师，用它的回答去训练小模型，从而低成本获得高质量的 SFT 数据。</p>

    <p>不过，SFT 有两个问题：一是人工编写的高质量数据昂贵，二是这种“直接模仿”未必能完全贴近人类偏好。于是基于人类反馈的强化学习（RLHF）登场——人类不需要写答案，只需对不同输出做出偏好选择，模型便能通过奖励信号调整生成方式，使输出更符合人类期望。</p>

    <p>进入推理模型阶段，SFT 与 RLHF 的结合催生了链式思维（CoT）等技术，即在回答之前显式生成推理步骤，让模型在内部组织更多的中间信息，从而提高最终答案的质量。DeepSeek 的成功正是强化学习+推理模型的代表。比如DeepSeek-R1-Zero 就完全跳过了 SFT，只依靠奖励信号引导模型形成规范化的思考过程，根据 prompt 分析和规划作答，效果相当惊艳。</p>

    <p>由于DeepSeek带来的震撼，我们基本上可以认为在生成性大语言模型的训练中，SFT变得更不重要了，强化学习就可以解决很多问题，虽然说强化学习训练效率没有SFT高。此外，在大语言模型的进一步发展中，我们也可以想见强化学习会越来越重要。如前面我们提到的AI的五个层次，在agent阶段，我们可以利用的数据就已经不多了，何况关于操作和行动的数据本身就很抽象，不容易获得，因此强化学习在agent阶段就越发重要了。在通向innovator的过程中，强化学习会变得更加重要，因为我们希望模型去发现一些人类不知道的知识，所以就不存在可以利用的训练数据，只能依赖人类的指导通向创新和发现。最近的Hierarchical Reasoning Model似乎给更进一步的发展提供了一些线索。HRM可以几乎不依赖数据，甚至不需要依赖预训练，但是可以解决一些需要推理的问题，例如解迷宫。如果这种能力可以得到更大的发展，我们似乎就有可能复现AlphaGo时刻，让模型通过强化学习，基于人类认可的规则，去自主探索知识的边界。</p>

    <p>不过，必须提醒一点：推理模型在生成“思维链”时，常常并非真的按人类逻辑思考，而是在生成符合训练分布的“看似合理的文字过程”。这意味着，即便推理链读起来像人类的分析过程，它也可能在最后一步给出荒谬答案。对社会科学研究者来说，这是一条重要的警告：我们不能简单地把模型的“推理文字”当作它真实的思维过程，更不能据此推断它的“认知模式”。</p>

    <h2 id="ops">大模型运营</h2>

    <p>大模型运营和模型能力本身无太大关系，一般社会科学研究者也很少会直接接触到，因为这一部分主要涉及到工程问题，即如何更有效率地完成训练、部署、运营大语言模型这项工程。了解这一过程有助于社会科学研究者对日常接触的大语言模型服务有更深刻的认识。</p>

    <p>我们先来讲离社会科学研究者距离最近的，即模型的运营。AI企业有了很好的基底模型之后，想要为大众提供服务就需要部署模型、保持模型可以被访问、遇到问题及时处理。我们从前端慢慢往后看。</p>

    <p>用户接触到的通常是聊天机器人的前端界面（大多是网页，也可能是应用程序）。当他们第一次打开页面时，前端会向 AI 公司服务器发一个初始化请求，拿到一个会话标识（session ID 或 token），用来区分这段对话和别人的对话，并且保留上下文，而不是用来确认你的真实身份。接下来，用户在文本框里输入 prompt，前端会把它和会话标识、模型参数等信息打包成一个 API 请求，通过互联网丢给 AI 公司的入口层，也就是 API Gateway。这里会先过一道“门卫”——验证你有没有权限、格式是不是对的、流量有没有超限——然后再交给全局负载均衡器。这个均衡器会权衡地理位置、各节点的实时负载、网络延迟等因素，把任务派到一个合适的推理集群；集群里的调度器再进一步，把任务交给一台空闲或者负载合适的 GPU 节点。为了更高效，中台在收到你的请求时，往往会等上几毫秒，把它和其他用户的请求拼成一个小批次（micro-batching），一次性送进模型，这样可以显著提高吞吐量。到了推理之前，系统会先去会话存储里捞出之前的对话内容，并且调用注意力键值缓存（KV cache）复用上一次推理的中间计算结果，这样就不用从头计算所有历史 token，响应速度会快得多。这里我们描述了一个三阶段模型，前端 - 中台 - 后端。</p>

    <p>事实上，实际的过程远比前端 - 中台 - 后端这个流程复杂。前端相对简单，主要负责与用户进行交互，接收和输出内容。到了中台，事情就变得稍微复杂了。因为AI公司需要最大化利用所有的计算资源，同时降低延迟，因此需要进行合理的任务组合，并且合理分配任务。这里面就首先涉及动态组合请求。因为大多数时候模型进行推理都是批处理，这样做可以提高处理效率。所以在中台接收到你的请求时，往往会预留一小段时间，这可能是若干毫秒，来组合你的请求和别的用户的请求，然后统一提交模型进行推理。在组合好请求之后，就会将请求统一发送到某个节点上的某个模型副本上去进行计算。</p>

    <p>这样任务就来到了后台。后台就涉及到更多的工程问题。我们自己在尝试本地部署大语言模型的时候所有事情都看上去非常直接，后台就是在用模型基于GPU进行推理。但是，当规模变大，模型服务商业化之后一切都变得复杂起来。后台管理，或者整个AI Infra设计需要考虑三个方面：计算、存储和通信。计算自不用多说，每一个推理过程都涉及到计算，这其中的关键在于如何最大提高每张卡的利用率，以免资源浪费。存储在AI Infra里成为一个问题，主要原因是在很多应用场景中，我们需要模型可以热启动，而不用重新计算用户之前输入的所有内容。为了实现这个，就需要存储上下文，和KV缓存。在AI Infra中，最主要的限制就是GPU显存。GPU显存相较于CPU显存就是非常有限，因此需要合理优化。因为GPU显存的客观限制，加上热启动的高要求，导致存储问题在AI Infra中是一个非常富有挑战性的问题。最后是通信。通常来讲，大语言模型的部署都依赖于多张GPU的联动，每张卡放上模型的一部分，这也就是所谓的model parallelism（比如张量并行或流水线并行,多头注意力机制天然就适应并行计算！）。这一过程无可避免的就需要GPU之间的通信，一张卡把它的计算结果告诉另一张卡，进行协作。考虑极端情况，如果卡间通信的摩擦为0，那么我们就可以进行完全的分布式架构，形成一张均质的分布式架构网。但现实并不是，卡间通信需要考虑两个阻碍：延迟和bandwidth。这就需要工程师根据客观状况去设计架构，开发传输协议了。</p>

    <p>上一段的介绍隐含了对AI Infra可扩展性，可分布性的介绍。接下来我们简要介绍一下对于所有Infra而言都极其重要的稳健性。稳健性保障了系统可以容纳局部错误，而不影响整体表现。比方说某个节点中的某块GPU出现了故障，系统需要这块GPU从可供调度的资源清单中去除，将新进的任务分配给节点中的其它模型副本。如果这块故障GPU负责存放模型的某一部分，它的瘫痪会导致整个模型的瘫痪，因此需要及时补上GPU，从中心存储中复制对应的模型参数，肩负起瘫痪的GPU的智能。或许不同AI公司资源redundant策略会有不同。目前看来，算力对于大多数公司来说都非常珍贵，不太可能会为了提高普通用户的微小体验，而大量布置redundant资源以提高容错率和稳健性。</p>

    <p>之前介绍的内容相对底层、抽象。下面我们来看看，在工程实践中这些底层功能是如何被工具实现的。宏观上来说，工程师们要做的事情很直接——在不同的计算节点上创建若干个相互独立的容器，在容器内部运行各自的模型副本，然后交给自动化部署与编排工具去管理。最常用的就是 Kubernetes（OpenAI 也在用），它就像整个系统的“工厂经理”，根据需求和资源情况安排每个容器在哪台机器上运行、什么时候启动、什么时候关闭。</p>

    <p>为什么要用容器和 K8s 呢？容器保证了环境一致性——无论是在研发人员的笔记本、测试服务器，还是在大规模的云集群中，模型都能在相同的运行环境中启动，不会出现“我本地能跑，线上却报错”的情况。容器还提供隔离性，让不同用户的推理任务互不干扰；同时具备可移植性，可以在不同机器、不同云厂商之间快速部署。K8s 则让这一切具备可伸缩性：当用户涌入时，它可以自动拉起更多模型副本来分担负载；当需求下降时，又能释放资源节省成本。可以把它类比成一个大型工厂，每台机器（节点）上划分出几个隔间（容器），每个隔间里都有一条完整的的小型生产线（模型副本），工厂经理（K8s）会根据订单量和机器情况安排生产线的数量和位置。</p>

    <p>不过，Kubernetes 并不是孤立工作的“大管家”，它周围还有很多“部门”在协作。比如，推理服务框架（如 vLLM和英伟达的TensorRT LLM）会负责优化 batch 处理、KV cache 管理、流式输出等底层推理细节；NVIDIA Device Plugin 或 GPU Operator 负责把 GPU 资源暴露给容器，让调度器能准确分配算力；分布式存储系统（Ceph、S3 等）负责存放模型权重和缓存，节点启动时能快速拉取；监控与告警系统（Prometheus + Grafana）则持续追踪延迟、吞吐量、GPU 利用率，一旦出现异常就提醒工程师。</p>

    <p>在部署中，还有一些不那么显而易见的工程挑战。首先是镜像拉取速度——大模型权重可能几十 GB，如果容器第一次启动时下载缓慢，整个服务就会被拖慢。其次是 GPU 拓扑调度——模型并行需要高速互联（如 NVLink、InfiniBand），调度器必须保证这些 GPU 在物理上是彼此可快速通信的，否则推理速度会大幅下降。再次是弹性与成本的平衡——多开副本可以降低延迟，但算力昂贵，不可能无限扩张。还有多版本共存的问题——不同用户可能调用不同版本的模型，需要同时部署、隔离运行。最后是热升级与零停机——如何在升级模型或推理框架的同时，不影响正在进行的对话，这需要非常细致的滚动更新策略。</p>

    <h2 id="security">额外话题：内容安全</h2>

    <p>最后简单聊聊内容安全的问题。这个问题在像中国这样对内容合规有较高要求的国家和地区比较重要。为了防止模型输出有害内容，理论上和实践中，从业者可以从数据、模型训练和模型部署层面去发力。我们知道，模型主要的能力其实来自于预训练阶段。高质量、大规模的预训练数据对预训练来说至关重要。很难想象中国的模型公司会为了内容合规在数据上投入大量精力进行筛选。根据中国关于生成式人工智能的有关规定，所谓有害内容只要低于4%即可。这个比例已经算是很宽松了，因为通常来讲常用的训练数据并不会包括这么高比例的有害数据，因为本身有害数据的比例就天然的不会很高，特别是在高质量数据中的比例。其次是在模型训练中进行干预。像DeepSeek模型，在预训练之后，主要依赖于强化学习来激活模型的能力，因此可以在强化学习阶段为模型注入偏好。与此同时，为了进一步合规，还可以利用未经审查的模型生成拒绝某些要求的推理过程，然后进行SFT，从而教会模型拒绝。最后是在模型部署和运营阶段。从目前的观察看，DeepSeek的网页聊天机器人至少有两重内容审查机制，首先是模型前审查，也即对输入的提示进行基于关键词的筛查。基于关键词的策略从实际观察和理性分析可以确认。因为基于关键词是最快速高效的审查机制，若是引入复杂的机制，会大大降低模型的反应速率。其次是事中的监督模型。我们可以从网页上看到，模型的回答是一个字一个字地输出，后台在返回输出时，可能同时至少在向两个主体进行发送，其一是用户，其二是监督模型。监督模型大概率是一个或者一组小型的分类器，来判断输出的内容是否有害，如果有害则指示前端撤回回答。有时候，答案会在完全输出之后被撤回，这有可能是事中监督模型依旧在起作用，或者是存在一个事后模型会对回答进行总体评估。不能排除存在事后模型的可能性，因为在全文输出后进行评估时，可以引入一个更强大的评估模型，而不用担心延迟的问题。</p>

    <h2 id="other">其它话题</h2>
    <p>人工智能的发展日新月异。还有许多值得社会科学研究者关注的话题，例如：</p>

    <li>机制解释</li>
    <li>Agent</li>
    <li>世界模型——基于信息论的模型理解</li>

    这些有趣的话题之后再找机会一一介绍。
    </div>
    <script>
        document.getElementById('sidebar-toggle').onclick = function() {
            var sidebar = document.getElementById('sidebar');
            sidebar.classList.toggle('open');
        };
    </script>
</body>
</html>